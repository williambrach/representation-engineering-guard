{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0803869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gqr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from repe import repe_pipeline_registry\n",
    "\n",
    "repe_pipeline_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda2aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, dtype=torch.float16, device_map=\"balanced_low_0\"\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = (\n",
    "    tokenizer.unk_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd891061",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gqr dataset\n",
    "train_dataset, test_dataset = gqr.load_train_dataset()\n",
    "test_dataset = gqr.load_id_test_dataset()\n",
    "ood_test_dataset = gqr.load_ood_test_dataset()\n",
    "\n",
    "# apply prompt template to the dataset\n",
    "train_dataset[\"text_template\"] = train_dataset[\"text\"].apply(\n",
    "    lambda x: template.format(query=x)\n",
    ")\n",
    "test_dataset[\"text_template\"] = test_dataset[\"text\"].apply(\n",
    "    lambda x: template.format(query=x)\n",
    ")\n",
    "ood_test_dataset[\"text_template\"] = ood_test_dataset[\"text\"].apply(\n",
    "    lambda x: template.format(query=x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e94c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_test_dataset = ood_test_dataset.sample(frac=0.1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=0.1, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 128\n",
    "\n",
    "CLASSES = sorted(train_dataset[\"label\"].unique())\n",
    "\n",
    "PIPELINE_KWARGS = {\n",
    "    \"rep_token\": -1,\n",
    "    \"hidden_layers\": list(range(-1, -model.config.num_hidden_layers, -1)),\n",
    "    \"n_difference\": 1,\n",
    "    \"direction_method\": \"pca\",\n",
    "    \"direction_finder_kwargs\": {\"n_components\": 1},\n",
    "    \"batch_size\": 4,\n",
    "}\n",
    "\n",
    "rep_reading_pipeline = pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "if rep_reading_pipeline.tokenizer.pad_token is None:\n",
    "    rep_reading_pipeline.tokenizer.pad_token = rep_reading_pipeline.tokenizer.eos_token\n",
    "rep_reading_pipeline.tokenizer.pad_token_id = (\n",
    "    rep_reading_pipeline.tokenizer.eos_token_id\n",
    ")\n",
    "train_data_all = []\n",
    "cls_readers = {}\n",
    "for cls in CLASSES:\n",
    "    # a. Sample positive examples for the current class\n",
    "    positive_df = train_dataset[train_dataset[\"label\"] == cls].sample(\n",
    "        n=N_SAMPLES, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    negative_classes = [c for c in CLASSES if c != cls]\n",
    "    n_neg_per_cls = N_SAMPLES // len(negative_classes)\n",
    "\n",
    "    negative_dfs = [\n",
    "        train_dataset[train_dataset[\"label\"] == neg_cls].sample(\n",
    "            n=n_neg_per_cls, random_state=RANDOM_SEED\n",
    "        )\n",
    "        for neg_cls in negative_classes\n",
    "    ]\n",
    "\n",
    "    negative_df = (\n",
    "        pd.concat(negative_dfs)\n",
    "        .sample(frac=1, random_state=RANDOM_SEED)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    pos_texts = positive_df[\"text_template\"].tolist()\n",
    "    neg_texts = negative_df[\"text_template\"].tolist()\n",
    "\n",
    "    train_data = [item for pair in zip(pos_texts, neg_texts, strict=True) for item in pair]\n",
    "\n",
    "    pos_labels = positive_df[\"label\"].tolist()\n",
    "    neg_labels = negative_df[\"label\"].tolist()\n",
    "    train_labels = [[p, n] for p, n in zip(pos_labels, neg_labels, strict=True)]\n",
    "\n",
    "    cls_readers[int(cls)] = rep_reading_pipeline.get_directions(\n",
    "        train_data, **PIPELINE_KWARGS\n",
    "    )\n",
    "    train_data_all.append(positive_df)\n",
    "cls_1, cls_2, cls_3 = cls_readers[0], cls_readers[1], cls_readers[2]\n",
    "data = []\n",
    "for i in PIPELINE_KWARGS['hidden_layers']:\n",
    "    data.append({\n",
    "        \"layer_id\": i,\n",
    "        \"class_0_direction\": cls_1.directions[i][0],\n",
    "        \"class_1_direction\": cls_2.directions[i][0],\n",
    "        \"class_2_direction\": cls_3.directions[i][0]\n",
    "    })\n",
    "pd.DataFrame(data).to_parquet(f\"{N_SAMPLES}_directions.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all = pd.concat(train_data_all).reset_index(drop=True)\n",
    "train_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affinity_scores(\n",
    "    text : str, pipeline : callable, class_readers : dict, rep_token : str, hidden_layers : list, decision_layer : int =-1\n",
    ") -> list:\n",
    "    class_scores = {}\n",
    "    query = [text]  # Pipeline expects a list of texts\n",
    "    class_inputs = {}\n",
    "    for cls, reader in class_readers.items():\n",
    "        # Get scores for the query against the current class's \"reader\"\n",
    "        outputs = pipeline(\n",
    "            query,\n",
    "            rep_reader=reader,\n",
    "            rep_token=rep_token,\n",
    "            hidden_layers=hidden_layers,\n",
    "            component_index=0,  # As used in the original snippet\n",
    "        )\n",
    "        scores = outputs[0][0]\n",
    "        inputs = outputs[0][1]\n",
    "\n",
    "        class_scores[cls] = [scores[(idx+1)*-1][0] for idx, layer in enumerate(hidden_layers)]\n",
    "        class_inputs[cls] = [inputs[idx][0] for idx, layer in enumerate(hidden_layers)]\n",
    "\n",
    "    return class_scores, class_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = []\n",
    "for _, row in tqdm(train_data_all.iterrows(), total=len(train_data_all)):\n",
    "    query_text = row[\"text_template\"]\n",
    "    true_label = row[\"label\"]\n",
    "\n",
    "    scores, inputs = get_affinity_scores(\n",
    "        text=query_text,\n",
    "        pipeline=rep_reading_pipeline,\n",
    "        class_readers=cls_readers,\n",
    "        rep_token=PIPELINE_KWARGS[\"rep_token\"],\n",
    "        hidden_layers=PIPELINE_KWARGS[\"hidden_layers\"],\n",
    "    )\n",
    "    train_dat.append({\n",
    "        \"label\" : true_label,\n",
    "        \"text\" : row['text'],\n",
    "        \"affinity_score_cls_0\" : scores[0],\n",
    "        \"hidden_states_cls_0\" : inputs[0],\n",
    "        \"affinity_score_cls_1\" : scores[1],\n",
    "        \"hidden_states_cls_1\" : inputs[1],\n",
    "        \"affinity_score_cls_2\" : scores[2],\n",
    "        \"hidden_states_cls_2\" : inputs[2],\n",
    "    })\n",
    "\n",
    "pd.DataFrame(train_dat).to_parquet(f\"{N_SAMPLES}_inference_train.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c525dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = []\n",
    "for _, row in tqdm(pd.concat([ood_test_dataset, test_dataset]).iterrows(), total=len(ood_test_dataset) + len(test_dataset)):\n",
    "    query_text = row[\"text_template\"]\n",
    "    true_label = row[\"label\"]\n",
    "\n",
    "    scores, inputs = get_affinity_scores(\n",
    "        text=query_text,\n",
    "        pipeline=rep_reading_pipeline,\n",
    "        class_readers=cls_readers,\n",
    "        rep_token=PIPELINE_KWARGS[\"rep_token\"],\n",
    "        hidden_layers=PIPELINE_KWARGS[\"hidden_layers\"],\n",
    "    )\n",
    "    validation_data.append({\n",
    "        \"label\" : true_label,\n",
    "        \"text\" : row['text'],\n",
    "        \"affinity_score_cls_0\" : scores[0],\n",
    "        \"hidden_states_cls_0\" : inputs[0],\n",
    "        \"affinity_score_cls_1\" : scores[1],\n",
    "        \"hidden_states_cls_1\" : inputs[1],\n",
    "        \"affinity_score_cls_2\" : scores[2],\n",
    "        \"hidden_states_cls_2\" : inputs[2],\n",
    "    })\n",
    "\n",
    "pd.DataFrame(validation_data).to_parquet(f\"{N_SAMPLES}_inference_test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation-engineering-guard (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
